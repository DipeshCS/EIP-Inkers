{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIPESH_PATEL_BatchA_GAN_CelebA.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "F4FKmGWxAkEl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Understanding GAN (with Keras)\n",
        "\n",
        "This tutorial aims to get you started with Generative Adversial Networks (GAN). We will first see what each term in GAN means. After that, we will start with implementation of one varient of GAN called [Deep Convolutional GAN](https://arxiv.org/abs/1511.06434) (DCGAN) along with explanation of each step in implementation. Throughout this tutorial, we will use [CelebA dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) to run our GAN models. \n",
        "\n",
        "\n",
        "## Outline of the Tutorial:\n",
        "\n",
        "\n",
        "* Understand GAN through basic concepts of Generative Models and Adversial Networks\n",
        "* DCGAN Architecture\n",
        "* Preparing CelebA dataset\n",
        "* Build Generator model for DCGAN\n",
        "* Build Discriminator model for DCGAN\n",
        "* Understand loss functions for both generator and discriminator.\n",
        "* Build Complete Training Model for DCGAN\n",
        "* Understand training limitations of naive DCGAN (for ex. mode collapse)\n",
        "* Learn improved techniques for training GAN with implementation\n",
        "* Conclusion & Future Scope\n",
        "\n",
        "\n",
        "## Dependencies:\n",
        "\n",
        "\n",
        "* tqdm==4.17.0\n",
        "* opencv_python==3.3.0.10\n",
        "* numpy==1.13.3\n",
        "* matplotlib==2.0.2\n",
        "* Keras==2.0.8\n",
        "* Tensorflow==1.3.0\n",
        "* h5py==2.7.0\n",
        "* parmap==1.5.1\n",
        "\n",
        "\n",
        "\n",
        "## What is GAN?\n",
        "\n",
        "\n",
        "To understand [GAN](https://arxiv.org/abs/1406.2661) in detail, Let's first try to understand what Generative Adversial Networks (GAN) term means. \n",
        "#### Generative models \n",
        "[Generative model](https://arxiv.org/pdf/1406.2661.pdf) refers to any model that takes a training set, consisting of samples drawn \n",
        "from a distribution **$p_{data}$** , and learns to represent an estimate of that distribution\n",
        "somehow. The result is a probability distribution **$p_{model}$**.  In case of GAN, it generates samples from estimated probability distribution **$p_{model}$**.\n",
        "\n",
        "#### Adversial Networks\n",
        "\n",
        "Adversial Networks are implemented by a system of two neural networks contesting with each other in a **zero-sum game** framework. Adversial training will allow two opponents to learn from each other's mistakes. In case of GAN, this zero-sum game will be in between generator and discriminator. \n",
        "\n",
        "Let us take a **real life analogy** to explain the concept:\n",
        "\n",
        "If you want to get better at a game, say chess; what would you do? You would play chess with an opponent better than you. Then you would analyze what you did wrong based on your moves (generator's training), what opponent did right (discriminator's training), and think on what could you do to beat opponent in the next game (generator's learning).\n",
        "\n",
        "You would repeat this step until you defeat the opponent. This concept can be used to build better learning models. So simply, for building a powerful hero (generator), we need a more powerful opponent (discriminator)!\n",
        "\n",
        "![GAN](https://deeplearning4j.org/img/gan_schema.png)\n",
        "\n",
        "\n",
        "Now, let's understand role of two players called generator and discriminator in terms of GAN. Let's say we trying to mimic celebrity faces based on CelebA dataset. Below mentioned are the steps that GAN takes :\n",
        "\n",
        "* Generator takes in random numbers as input and returns an image.\n",
        "* Generated image by generator is fed into the discriminator along with a batch of images taken from the real dataset which GAN is trying to mimic.\n",
        "* Discriminator takes in both real and fake images and returns probabilities, a number between 0 and 1, with 1 representing a prediction of authenticity and 0 representing fake.\n",
        "\n",
        "So we have a double feedback loop in GAN:\n",
        "\n",
        "* The discriminator is in a feedback loop with real dataset.\n",
        "* The generator is in a feedback loop with the discriminator using it's output to improve generator model.\n",
        "\n",
        "\n",
        "Since we have a basic understanding of how GAN works, Let's move on to one particular varient of GAN called Deep Convolutional GAN (DCGAN).\n",
        "\n",
        "\n",
        "## DCGAN Architecure\n",
        "\n",
        "![DCGAN](images/DCGANArch.png)\n",
        "\n",
        "[DCGAN](https://arxiv.org/abs/1511.06434) stands for \"Deep Convolution GAN\". Below mentioned are the key insights from DCGAN architecture.\n",
        "* Network Structure is based on idea of all-convolutional network. It has no pooling layers. When generator needs to increase the spatial\n",
        "dimensionality of the representation, it can use transposed convolution with a\n",
        "stride greater than 1.\n",
        "* Architecture uses batch normalization in all layers of discriminator and generator except first layer of discriminator and last layer of generator.\n",
        "* Architecture uses Adam optimizer rather than SGD with momentum.\n",
        "\n",
        "Now, Let's start implementing DCGAN architecture before dwelling more into GAN.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kjxHtWH_n5i2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from scipy.misc import imresize\n",
        "from glob import glob\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import keras\n",
        "import h5py\n",
        "import keras.backend as K\n",
        "!pip install parmap\n",
        "import parmap\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.layers import Input, Concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Flatten, Dense, Dropout, Activation, Lambda, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Deconv2D, ZeroPadding2D, UpSampling2D, Conv2DTranspose\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam,SGD\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nloUFJfFn8PR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading & Preprocessing CelebA dataset\n",
        "\n",
        "**CelebFaces Attributes Dataset (CelebA)** is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. In our case, we will not require attribute annotations. We will load celebrity images and fed it into discriminator model after pre-processing.\n",
        "\n",
        "### Steps to download CelebA Dataset\n",
        "\n",
        "- Go to http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
        "- In the Downloads section, select Align&Cropped images.\n",
        "- In the dropbox page that follows, download the Anno, Eval and Img folders.\n",
        "- Extract the zip files.\n",
        "\n",
        "You should have the following folder structure:\n",
        "\n",
        "    ├── Anno\n",
        "        ├── list_attr_celeba.txt  \n",
        "        ├── list_bbox_celeba.txt  \n",
        "        ├── list_landmarks_align_celeba.txt  \n",
        "        ├── list_landmarks_celeba.txt\n",
        "    ├── Eval\n",
        "        ├──list_eval_partition.txt\n",
        "    ├── img_align_celeba\n",
        "        ├──lots of images\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Dwfwmjzgm_9U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once you have dataset in place on local system, load all the filenames.Once you execute code snippet below, it will ask to upload images. Based on number of images you upload, adjust **chunk_size** variable,  which helps us to divide data into fixed size chunks to pre-process each chunk in parallel."
      ]
    },
    {
      "metadata": {
        "id": "wMIOIQ36kcG-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn]))\n",
        "filenames = np.array(glob('*.jpg'))\n",
        "print(filenames)\n",
        "num_files = len(filenames)\n",
        "chunk_size = 5\n",
        "num_chunks = num_files / chunk_size\n",
        "print(num_chunks)\n",
        "arr_chunks = np.array_split(np.arange(num_files), num_chunks)\n",
        "print(arr_chunks)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDdjA6YGm0Rb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's define function which will take image and output size as input parameters. This function will mainly perform two pre-processing steps:\n",
        "* Slice image to center around face\n",
        "* Reduce image size to the size provided as input parameter of function (64 in this case)"
      ]
    },
    {
      "metadata": {
        "id": "Qa5Yis2SkK7g",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def preprocess_image(img_path, size=64):\n",
        "    img_color = cv2.imread(img_path)\n",
        "    img_color = img_color[:, :, ::-1]\n",
        "    # Slice image to center around face\n",
        "    img_color = img_color[30:-30, 20:-20, :]\n",
        "    img_color = cv2.resize(img_color, (size, size), interpolation=cv2.INTER_AREA)\n",
        "    img_color = img_color.reshape((1, size, size, 3)).transpose(0, 3, 1, 2)\n",
        "    print(img_color.shape)\n",
        "    return img_color"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8hli4Jdhpltq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's create training dataset of face images using above defined preprocess_image function. \n",
        "\n",
        "We will use HDF5 file to create single dataset using [h5py](https://www.h5py.org/) library. An HDF5 file is a container for two kinds of objects: datasets, which are array-like collections of data, and groups, which are folder-like containers that hold datasets and other groups.\n",
        "\n",
        "We will use permap library to preprocesses images parallelly in each chunk.\n",
        "\n",
        "In the end, we will append preprocessed images to dataset."
      ]
    },
    {
      "metadata": {
        "id": "9FzjNiGYplAo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with h5py.File(\"CelebA_sample100.h5\", \"w\") as hfw:\n",
        "  X_train = hfw.create_dataset(\"data\",\n",
        "                                  (0, 3, 64, 64),\n",
        "                                  maxshape=(None, 3, 64, 64),\n",
        "                                  dtype=np.uint8)\n",
        "\n",
        "  for chunk_idx in tqdm(arr_chunks):\n",
        "      print(chunk_idx)\n",
        "      list_img_path = filenames[chunk_idx].tolist()\n",
        "      output = parmap.map(preprocess_image, list_img_path, 64, pm_parallel=True)\n",
        "      arr_img_color = np.concatenate(output, axis=0)\n",
        "      X_train.resize(X_train.shape[0] + arr_img_color.shape[0], axis=0)\n",
        "      X_train[-arr_img_color.shape[0]:] = arr_img_color.astype(np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "db_crhZtw3fm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we will see later on, the generator is using $tanh$ activation, for which we need to normalize the image data into the range between -1 and 1. We will add inverse normalization function as well.\n"
      ]
    },
    {
      "metadata": {
        "id": "o2MaO-fTwGMW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def normalization(X):\n",
        "    return X / 127.5 - 1\n",
        "  \n",
        "def inverse_normalization(X):\n",
        "    return (X + 1.) / 2."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d_jcZm24ylpw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with h5py.File(\"CelebA_sample100.h5\", \"r\") as hf:\n",
        "    X_real_train = hf[\"data\"][:].astype(np.float32).transpose(0, 2, 3, 1)\n",
        "    X_real_train = normalization(X_real_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wcCJ0RALnfqf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Generator model for DCGAN\n",
        "\n",
        "![Generator](https://cdn-images-1.medium.com/max/1600/1*Tv7wjpBTB0Pg6rWfLm4YSA.png) \n",
        "\n",
        "The generator takes a latent sample (100 randomly generated numbers) and produces a color face image that should look like one from the CelebA dataset. \n",
        "\n",
        "Generator model shown in image above is self-explanatory if you understand Convolution NN. Since this tutorial aims to explore GAN, we will not cover basics of convolution. If you want more details about Convolution based NNs, refer [this](http://cs231n.github.io/convolutional-networks/) tutorial.\n",
        "\n",
        "#### Few highlights from Generator Model :\n",
        "\n",
        " * We will use $tanh$ activation at last layer of model, as suggested [here](https://github.com/soumith/ganhacks).\n",
        " * We will use transposed convolution instead of normal convolution to achieve transformation in opposite direction of normal convolution while maintaining connectivity patterns compatible with convolution. For more details, refer [this](https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0) tutorial. \n",
        " \n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "M7_-YZpOoRHF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def generator_deconv(noise_dim, img_dim, batch_size):\n",
        "\n",
        "    s = img_dim[1]\n",
        "    f = 1024\n",
        "    start_dim = int(s / 16)\n",
        "    nb_upconv = 4\n",
        "\n",
        "    reshape_shape = (start_dim, start_dim, f)\n",
        "    output_channels = img_dim[-1]\n",
        "\n",
        "    gen_input = Input(shape=noise_dim, name=\"generator_input\")\n",
        "\n",
        "    x = Dense(f * start_dim * start_dim, input_dim=noise_dim)(gen_input)\n",
        "    x = Reshape(reshape_shape)(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    # Transposed conv blocks\n",
        "    for i in range(nb_upconv - 1):\n",
        "        nb_filters = int(f / (2 ** (i + 1)))\n",
        "        s = start_dim * (2 ** (i + 1))\n",
        "        o_shape = (batch_size, s, s, nb_filters)\n",
        "        x = Conv2DTranspose(nb_filters, (3, 3), strides=(2, 2), padding=\"same\")(x)\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "\n",
        "    # Last block\n",
        "    s = start_dim * (2 ** (nb_upconv))\n",
        "    o_shape = (batch_size, s, s, output_channels)\n",
        "    x = Conv2DTranspose(output_channels, (3, 3), strides=(2, 2), padding=\"same\")(x)\n",
        "    x = Activation(\"tanh\")(x)\n",
        "\n",
        "    generator_model = Model(inputs=[gen_input], outputs=[x])\n",
        "\n",
        "    return generator_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0maQ4J80X4Zs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build Discriminator model for DCGAN\n",
        "\n",
        "Discriminator model for DCGAN consist of 4 blocks of layers which in turn contains convolution, BatchNorm and LeakyReLU layers. You can directly feed output of last block to Dense block which will map it to final probability distribution. Discriminator model used here is just traditional convolutional NN.\n",
        "\n",
        "But then, You might wonder about a role of function named **minb_disc** which is a part of discriminator model. Well, This function is trying to reduce/solve a problem called **Mode collapse**. Mode collapse occurs while training GAN itself. \n",
        "\n",
        "### Mode Collapse \n",
        "During the training process of GAN, generator may collapse to parameter settings/values where it mostly/always produces same set of generated images. When collapse to single mode is about to occur, gradient of the discriminator may point in similar directions because discriminator is not able to distinguish between samples since it processes each generator sample independently.\n",
        "\n",
        "In such a scenario, the generator will exhibit really poor diversity among generated samples, which limits the usefulness of the learnt GAN. \n",
        "\n",
        "One solution to reduce possibility of mode collapse is to look and process multiple images at a time instead of just one image. To achieve this, we will use a technique called mini-batch discrimination which is explained below:\n",
        "\n",
        "### Mini-bach Descrimination\n",
        "[Mini-batch descrimination](https://arxiv.org/abs/1606.03498) allows discriminator model to look at multiple examples in combination, rather than in isolation which could potentially help avoid collapse of the generator. With minibatch discrimination, the discriminator is able to digest the relationship between training data points in one batch, instead of processing each point independently.\n",
        "\n",
        "In one minibatch layer, we approximate the closeness between every pair of samples, $c(x_i,x_j)$, and get the overall summary of one data point by summing up how close it is to other samples in the same batch, $o(x_i)=∑_jc(x_i,x_j)$. Then $o(x_i)$ is concatenated to output of previous layer to mini-batch layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sPb-sl8BZvTj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def DCGAN_discriminator(noise_dim, img_dim):\n",
        "\n",
        "    disc_input = Input(shape=img_dim, name=\"discriminator_input\")\n",
        "\n",
        "    list_f = [64, 128, 256]\n",
        "\n",
        "    # First conv\n",
        "    x = Conv2D(32, (3, 3), strides=(2, 2), name=\"disc_Conv2D_1\", padding=\"same\")(disc_input)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Next convs\n",
        "    for i, f in enumerate(list_f):\n",
        "        name = \"disc_Conv2D_%s\" % (i + 2)\n",
        "        x = Conv2D(f, (3, 3), strides=(2, 2), name=name, padding=\"same\")(x)\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "        x = LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    def minb_disc(x):\n",
        "        diffs = K.expand_dims(x, 3) - K.expand_dims(K.permute_dimensions(x, [1, 2, 0]), 0)\n",
        "        abs_diffs = K.sum(K.abs(diffs), 2)\n",
        "        x = K.sum(K.exp(-abs_diffs), 2)\n",
        "        return x\n",
        "\n",
        "    def lambda_output(input_shape):\n",
        "        return input_shape[:2]\n",
        "\n",
        "    num_kernels = 100\n",
        "    dim_per_kernel = 5\n",
        "\n",
        "    M = Dense(num_kernels * dim_per_kernel, use_bias=False, activation=None)\n",
        "    MBD = Lambda(minb_disc, output_shape=lambda_output)\n",
        "\n",
        "    x_mbd = M(x)\n",
        "    x_mbd = Reshape((num_kernels, dim_per_kernel))(x_mbd)\n",
        "    x_mbd = MBD(x_mbd)\n",
        "    x = Concatenate(axis=-1)([x, x_mbd])\n",
        "\n",
        "    x = Dense(2, activation='softmax', name=\"disc_dense_2\")(x)\n",
        "\n",
        "    discriminator_model = Model(inputs=[disc_input], outputs=[x])\n",
        "\n",
        "    return discriminator_model\n",
        "\n",
        "\n",
        "def DCGAN(generator, discriminator_model, noise_dim, img_dim):\n",
        "\n",
        "    noise_input = Input(shape=noise_dim, name=\"noise_input\")\n",
        "\n",
        "    generated_image = generator(noise_input)\n",
        "    DCGAN_output = discriminator_model(generated_image)\n",
        "\n",
        "    DCGAN = Model(inputs=[noise_input],\n",
        "                  outputs=[DCGAN_output],\n",
        "                  name=\"DCGAN\")\n",
        "\n",
        "    return DCGAN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xqrooWEEmEfP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have defined generator and discriminator models till now. Before we start with training module, we will define some utility functions that will be useful in training process.\n",
        "\n",
        "Most of the utility functions such as **gen_batch, sample_noise** etc. are self-explanatory. One utility function that requires our attention is **get_disc_batch**. \n",
        "\n",
        "**get_disc_batch** function basically helps us in generation batches for discriminator model training.  Here, we will use only generated or real images per batch alternatively as suggested [here](https://github.com/soumith/ganhacks).  As you can see in function, there are two cases based on value of batch_counter :\n",
        "1) In case of fake images, we will generate random vector and get fake image from generator model. We will set label as fake (0).\n",
        "2) In case of real images, we will use actual images and set label as real (1).\n",
        "\n",
        "You might notice few things like label_smoothing & label_flipping etc. Let's understand them one by one.\n",
        "\n",
        "#### One-Sided Label Smoothing\n",
        "label smoothing basically suggests to replace 0 & 1 targets for the classifier (discriminator in our case) with smoothed values like $0.1$ & $0.9$ respectively.  Label smoothing shown to reduce vulnerability of neural networks in case of adversial training as suggested [here](https://arxiv.org/abs/1606.03498).  Note that, we will only smooth positive labels for discriminator.\n",
        "\n",
        "#### Label Flipping\n",
        "Label flipping basically suggests to occasionally flip the labels for discriminator. It helps in stabilizing training for GAN as suggested [here](https://github.com/soumith/ganhacks). Also, we will flip labels based on binomial distribution as shown in function.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UhTPhtnbnRm5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def gen_batch(X, batch_size):\n",
        "\n",
        "    while True:\n",
        "        idx = np.random.choice(X.shape[0], batch_size, replace=False)\n",
        "        yield X[idx]\n",
        "\n",
        "\n",
        "def sample_noise(noise_scale, batch_size, noise_dim):\n",
        "\n",
        "    return np.random.normal(scale=noise_scale, size=(batch_size, noise_dim[0]))\n",
        "\n",
        "\n",
        "def get_disc_batch(X_real_batch, generator_model, batch_counter, batch_size, noise_dim,\n",
        "                   noise_scale=0.5, label_smoothing=False, label_flipping=0):\n",
        "\n",
        "    # Create X_disc: alternatively only generated or real images\n",
        "    if batch_counter % 2 == 0:\n",
        "        # Pass noise to the generator\n",
        "        noise_input = sample_noise(noise_scale, batch_size, noise_dim)\n",
        "        # Produce an output\n",
        "        X_disc = generator_model.predict(noise_input)\n",
        "        y_disc = np.zeros((X_disc.shape[0], 2), dtype=np.uint8)\n",
        "        y_disc[:, 0] = 1\n",
        "\n",
        "        if label_flipping > 0:\n",
        "            p = np.random.binomial(1, label_flipping)\n",
        "            if p > 0:\n",
        "                y_disc[:, [0, 1]] = y_disc[:, [1, 0]]\n",
        "\n",
        "    else:\n",
        "        X_disc = X_real_batch\n",
        "        y_disc = np.zeros((X_disc.shape[0], 2), dtype=np.uint8)\n",
        "        if label_smoothing:\n",
        "            y_disc[:, 1] = np.random.uniform(low=0.9, high=1, size=y_disc.shape[0])\n",
        "        else:\n",
        "            y_disc[:, 1] = 1\n",
        "\n",
        "        if label_flipping > 0:\n",
        "            p = np.random.binomial(1, label_flipping)\n",
        "            if p > 0:\n",
        "                y_disc[:, [0, 1]] = y_disc[:, [1, 0]]\n",
        "\n",
        "    return X_disc, y_disc\n",
        "\n",
        "\n",
        "def get_gen_batch(batch_size, noise_dim, noise_scale=0.5):\n",
        "\n",
        "    X_gen = sample_noise(noise_scale, batch_size, noise_dim)\n",
        "    y_gen = np.zeros((X_gen.shape[0], 2), dtype=np.uint8)\n",
        "    y_gen[:, 1] = 1\n",
        "\n",
        "    return X_gen, y_gen\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AsYi3g37vDrt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training GAN\n",
        "\n",
        "Since we have models and utility functions in place, we can start building training model for GAN. GAN training is most important & challenging part of GAN training. We will first go through loss functions for GAN. Later, we will cover training flow and important highlights in GAN training.\n",
        "\n",
        "#### Loss Functions\n",
        "First, we will start with discriminator loss function. It is just standard cross-entropy cost that is minimized.\n",
        "\n",
        "$\\qquad$$\\qquad$$J_{D}(D,G) = - \\sum_{x∼p_{data}} [log D(x)] -   \\sum_{z∼p_{z}} [log(1 − D(G(z)))]  $\n",
        "\n",
        "where $D$ is Discriminator function and $G$ is Generator function. Also, $p_{data}$ refers to real data distribution and $p_{z}$ refers to generated data distribution.\n",
        "\n",
        "\n",
        "The only difference, from standard cross-entropy, is that classifier is training on two mini-batches of dataset instead one; dataset coming from generator where labels are 0 and dataset coming from real images where labels are 1 for all examples. Most of GAN variants use above loss functions for discriminator till now. They vary in terms of generator loss function which we will see now.\n",
        "\n",
        "\n",
        "The theoretical analysis in [original GAN paper](https://arxiv.org/abs/1406.2661) is based on a **zero-sum game** in which, generator attempts to generate sample that have low probability of being fake, by minimizing the below mentioned objective function.\n",
        "\n",
        "$\\qquad$$\\qquad$$J_{G}(G) =  \\sum_{z∼p_{z}} [log(1 − D(G(z)))]  $\n",
        "\n",
        "\n",
        "However, loss function mentioned above may not provide sufficient gradient for generator to learn well. When G is poor early in training, D rejects generated samples with high confidence resulting in saturation of above loss function. \n",
        "\n",
        "In practice, [original GAN paper](https://arxiv.org/abs/1406.2661) recommends alternative loss function which ensures that generated sample have high probability of being real, by minimizing below mentioned alternative objective function:\n",
        "\n",
        "$\\qquad$$\\qquad$$J_{G}(G) = - \\sum_{z∼p_{z}} [logD(G(z))]  $\n",
        "\n",
        "\n",
        "\n",
        "We will use non-saturating (due to non-saturating behaviour of gradient) objective shown above in our implementation. For more details about loss functions of GAN, refer [this](https://arxiv.org/abs/1701.00160) tutorial.\n",
        "\n",
        "#### Optimality for GAN training\n",
        "\n",
        "When generated probability distribution is equals to real probability distribution, GAN is trained to generate images as close as real images. Essentially, when discriminator is optimal, loss function of GAN quantifies the similarity between the generative data distribution pg and the real sample distribution pr by **Jensen–Shannon divergence**. For more details about mathematical proof, refer [this](https://arxiv.org/abs/1406.2661) paper.\n",
        "\n",
        "\n",
        "#### Training Highlights and Process\n",
        "\n",
        "* We will use SGD optimizer for training discriminator and ADAM optimization for training DCGAN model as suggested [here](https://github.com/soumith/ganhacks).\n",
        "* We will first train discriminator alone on batch of generated and real images. After that, We will train freeze discriminator and train DCGAN model to train generator based on feedback from discriminator.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CJ1eL73nc1wq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def train(**kwargs):\n",
        "    \"\"\"\n",
        "    Train model\n",
        "    Load the whole train data in memory for faster operations\n",
        "    args: **kwargs (dict) keyword arguments that specify the model hyperparameters\n",
        "    \"\"\"\n",
        "\n",
        "    # Roll out the parameters\n",
        "    batch_size = kwargs[\"batch_size\"]\n",
        "    n_batch_per_epoch = kwargs[\"n_batch_per_epoch\"]\n",
        "    nb_epoch = kwargs[\"nb_epoch\"]\n",
        "    model_name = kwargs[\"model_name\"]\n",
        "    image_data_format = kwargs[\"image_data_format\"]\n",
        "    img_dim = kwargs[\"img_dim\"]\n",
        "    label_smoothing = kwargs[\"label_smoothing\"]\n",
        "    label_flipping = kwargs[\"label_flipping\"]\n",
        "    noise_scale = kwargs[\"noise_scale\"]\n",
        "    use_mbd = kwargs[\"use_mbd\"]\n",
        "    epoch_size = n_batch_per_epoch * batch_size\n",
        "\n",
        "    # Setup environment (logging directory etc)\n",
        "    # general_utils.setup_logging(model_name)\n",
        "\n",
        "    img_dim = X_real_train.shape[-3:]\n",
        "    noise_dim = (100,)\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Create optimizers\n",
        "        opt_dcgan = Adam(lr=1E-3, beta_1=0.5, beta_2=0.999, epsilon=1e-08)\n",
        "        opt_discriminator = SGD(lr=1E-3, momentum=0.9, nesterov=True)\n",
        "\n",
        "        # Load generator model\n",
        "        generator_model = generator_deconv(noise_dim,\n",
        "                                      img_dim,\n",
        "                                      batch_size)\n",
        "        # Load discriminator model\n",
        "        discriminator_model = DCGAN_discriminator(noise_dim,\n",
        "                                          img_dim)\n",
        "\n",
        "        generator_model.compile(loss='mse', optimizer=opt_discriminator)\n",
        "        discriminator_model.trainable = False\n",
        "\n",
        "        DCGAN_model = DCGAN(generator_model,\n",
        "                                   discriminator_model,\n",
        "                                   noise_dim,\n",
        "                                   img_dim)\n",
        "\n",
        "        loss = ['binary_crossentropy']\n",
        "        loss_weights = [1]\n",
        "        DCGAN_model.compile(loss=loss, loss_weights=loss_weights, optimizer=opt_dcgan)\n",
        "\n",
        "        discriminator_model.trainable = True\n",
        "        discriminator_model.compile(loss='binary_crossentropy', optimizer=opt_discriminator)\n",
        "\n",
        "        gen_loss = 100\n",
        "        disc_loss = 100\n",
        "\n",
        "        # Start training\n",
        "        print(\"Start training\")\n",
        "        for e in range(nb_epoch):\n",
        "            # Initialize progbar and batch counter\n",
        "            #progbar = generic_utils.Progbar(epoch_size)\n",
        "            batch_counter = 1\n",
        "            start = time.time()\n",
        "\n",
        "            for X_real_batch in gen_batch(X_real_train, batch_size):\n",
        "\n",
        "                # Create a batch to feed the discriminator model\n",
        "                X_disc, y_disc = get_disc_batch(X_real_batch,\n",
        "                                                           generator_model,\n",
        "                                                           batch_counter,\n",
        "                                                           batch_size,\n",
        "                                                           noise_dim,\n",
        "                                                           noise_scale=noise_scale,\n",
        "                                                           label_smoothing=label_smoothing,\n",
        "                                                           label_flipping=label_flipping)\n",
        "\n",
        "                # Update the discriminator\n",
        "                disc_loss = discriminator_model.train_on_batch(X_disc, y_disc)\n",
        "\n",
        "                # Create a batch to feed the generator model\n",
        "                X_gen, y_gen = get_gen_batch(batch_size, noise_dim, noise_scale=noise_scale)\n",
        "\n",
        "                # Freeze the discriminator\n",
        "                discriminator_model.trainable = False\n",
        "                gen_loss = DCGAN_model.train_on_batch(X_gen, y_gen)\n",
        "                # Unfreeze the discriminator\n",
        "                discriminator_model.trainable = True\n",
        "\n",
        "                batch_counter += 1\n",
        "                #progbar.add(batch_size, values=[(\"D logloss\", disc_loss),\n",
        "                #                               (\"G logloss\", gen_loss)])\n",
        "\n",
        "                # Save images for visualization\n",
        "#                 if batch_counter % 100 == 0:\n",
        "#                     data_utils.plot_generated_batch(X_real_batch, generator_model,\n",
        "#                                                     batch_size, noise_dim, image_data_format)\n",
        "\n",
        "                if batch_counter >= n_batch_per_epoch:\n",
        "                    break\n",
        "\n",
        "            print(\"\")\n",
        "            print('Epoch %s/%s, Time: %s' % (e + 1, nb_epoch, time.time() - start))\n",
        "\n",
        "            if e % 5 == 0:\n",
        "                gen_weights_path = os.path.join('%s_gen_weights_epoch%s.h5' % (model_name, e))\n",
        "                generator_model.save_weights(gen_weights_path, overwrite=True)\n",
        "\n",
        "                disc_weights_path = os.path.join('%s_disc_weights_epoch%s.h5' % (model_name, e))\n",
        "                discriminator_model.save_weights(disc_weights_path, overwrite=True)\n",
        "\n",
        "                DCGAN_weights_path = os.path.join('%s_DCGAN_weights_epoch%s.h5' % (model_name, e))\n",
        "                DCGAN_model.save_weights(DCGAN_weights_path, overwrite=True)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cvjdvjOlS3ZL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Change parameters based on size of training dataset\n",
        "\n",
        "d_params = {\"mode\": \"train_GAN\",\n",
        "              \"batch_size\": 2,\n",
        "              \"n_batch_per_epoch\": 5,\n",
        "              \"nb_epoch\": 50,\n",
        "              \"model_name\": \"CNN\",\n",
        "              \"do_plot\": False,\n",
        "              \"image_data_format\": \"channels_last\",\n",
        "              \"bn_mode\": 2,\n",
        "              \"img_dim\": 64,\n",
        "              \"label_smoothing\": True,\n",
        "              \"label_flipping\": True,\n",
        "              \"noise_scale\": 0.5,\n",
        "              \"use_mbd\": True,\n",
        "            }\n",
        "\n",
        "# Launch training\n",
        "train(**d_params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "boqIxwQaMlZF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results & Future Scope \n",
        "\n",
        "![GANResults](images/GANResults.png)\n",
        "\n",
        "First two rows of results represents GAN generated images and last two rows of results represents real images from CelebA dataset. \n",
        "\n",
        "## Future Scope \n",
        "\n",
        "#### Use Better Metric of Distribution Similarity\n",
        "Loss function of the vanilla GAN measures the JS divergence between the distributions of real data distribution and generated distribution. This metric fails to provide a meaningful value when two distributions are disjoint, which is frequent case for GAN.\n",
        "\n",
        "To overcome this, [Wasserstein metric](https://arxiv.org/abs/1701.07875) is proposed, which has much smoother value even when distributions are disjoint. If you are further interested in Wasserstein based GAN, you can explore [WGAN with gradient penalties](https://arxiv.org/abs/1704.00028) & [DRAGAN](https://arxiv.org/abs/1705.07215)\n",
        "\n",
        "#### Energy-based GANs\n",
        "\n",
        "Main idea behind energy-based GAN is to consider discriminator as energy function which assigns low energies to regions near data manifolds and higher energies in other regions. One such GAN called [EBGAN](https://arxiv.org/abs/1609.03126) uses auto-encoder as discriminator with energy being reconstruction error. \n",
        "\n",
        "[BEGAN](https://arxiv.org/abs/1703.10717), which was state-of-the art for generating realistic faces, aims to compare auto-encoders' reconstruction losses of real and generated images instead of comparing direct data distribution.\n",
        "\n",
        "#### GAN on Higher-resolution Images\n",
        "\n",
        "In this tutorial, We worked on face images of low resolution ($64*64$). [Progressive GAN](https://arxiv.org/abs/1710.10196) is able to generate HD face images of size $1024*1024$. The key idea is to grow generator and discriminator progressively from low resolution images.\n",
        "\n",
        "Lastly, If you are interested about equilibrium of GAN training, refer [this](https://arxiv.org/abs/1710.08446) awesome paper.\n",
        "\n",
        "\n"
      ]
    }
  ]
}